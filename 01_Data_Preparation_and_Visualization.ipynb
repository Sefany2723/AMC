{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81acd40",
   "metadata": {},
   "source": [
    "# 01 — Data Preparation and Visualization (RadioML 2016.10b)\n",
    "\n",
    "This notebook loads, explores, visualizes, preprocesses, splits, and saves the RadioML 2016.10b dataset for Automatic Modulation Classification (AMC). It follows the project standards: English Markdown, strict plot styling (save first then show), reproducibility, and no data leakage.\n",
    "\n",
    "Outputs: preprocessed NumPy arrays in 1D and 2D shapes, along with fitted preprocessing objects saved for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65258de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, global style, seeds, and GPU check\n",
    "import os, sys, json, pickle, math, random, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotly optional (for advanced visualizations if desired)\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Style and aesthetics\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f'GPU detected: {gpus}')\n",
    "else:\n",
    "    print('No GPU detected by TensorFlow.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bfafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and Google Drive mount (for Colab)\n",
    "from datetime import datetime\n",
    "\n",
    "# Define project root and data paths (per spec)\n",
    "PROJECT_ROOT_PATH = '/content/drive/MyDrive/DTVT_IUH_2025/AMC_RML2016_10b/'\n",
    "RAW_DATA_PATH = '/content/drive/MyDrive/RML2016.10b.dat'\n",
    "\n",
    "# Attempt to mount Google Drive in Colab; ignore errors outside Colab\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    print('Google Drive mounted.')\n",
    "except Exception as e:\n",
    "    print('Not running in Colab or Drive mount skipped. Proceeding with local paths if set.')\n",
    "\n",
    "# Construct subfolder paths\n",
    "PROCESSED_DATA_PATH = os.path.join(PROJECT_ROOT_PATH, 'data/')\n",
    "VISUALIZATIONS_PATH = os.path.join(PROJECT_ROOT_PATH, 'visualizations/')\n",
    "PREPROCESSING_OBJECTS_PATH = os.path.join(PROJECT_ROOT_PATH, 'preprocessing_objects/')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "for d in [PROJECT_ROOT_PATH, PROCESSED_DATA_PATH, VISUALIZATIONS_PATH, PREPROCESSING_OBJECTS_PATH]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "print('Directories ensured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RadioML 2016.10b dataset and convert to DataFrame\n",
    "def load_radioml_2016_10b(path):\n",
    "    \"\"\"\n",
    "    Load the RadioML 2016.10b .dat file using pickle.\n",
    "    Parameters:\n",
    "        path (str): Absolute path to RML2016.10b.dat\n",
    "    Returns:\n",
    "        dict: Keys are (modulation, snr), values are np.ndarray of shape (N, 2, 128)\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "    return data\n",
    "\n",
    "def to_dataframe(data_dict):\n",
    "    \"\"\"\n",
    "    Convert RadioML dict to a pandas DataFrame with columns: signal (object), modulation (str), snr (int).\n",
    "    Parameters:\n",
    "        data_dict (dict): {(modulation:str, snr:int): np.ndarray(N, 2, 128)}\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for (mod, snr), arr in data_dict.items():\n",
    "        for i in range(arr.shape[0]):\n",
    "            records.append({\n",
    "                'signal': arr[i],\n",
    "                'modulation': str(mod),\n",
    "                'snr': int(snr)\n",
    "            })\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    return df\n",
    "\n",
    "data_dict = load_radioml_2016_10b(RAW_DATA_PATH)\n",
    "df = to_dataframe(data_dict)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print('\\nDataFrame info:')\n",
    "df.info()\n",
    "\n",
    "mods = sorted(df['modulation'].unique().tolist())\n",
    "snrs = sorted(df['snr'].unique().tolist())\n",
    "print('\\nUnique modulations:', mods)\n",
    "print('Unique SNRs:', snrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution analysis and countplot\n",
    "mod_counts = df['modulation'].value_counts().sort_index()\n",
    "snr_counts = df['snr'].value_counts().sort_index()\n",
    "pivot_counts = pd.pivot_table(df, index='modulation', columns='snr', values='signal', aggfunc='count', fill_value=0)\n",
    "\n",
    "print('Samples per Modulation:')\n",
    "print(mod_counts)\n",
    "print('\\nSamples per SNR:')\n",
    "print(snr_counts)\n",
    "print('\\nSamples per (Modulation, SNR):')\n",
    "display(pivot_counts)\n",
    "\n",
    "# Countplot of modulation distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.countplot(x='modulation', data=df, order=mods, palette='tab10')\n",
    "plt.title('Distribution of Modulation Types', fontsize=12)\n",
    "plt.xlabel('Modulation')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.margins(x=0.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VISUALIZATIONS_PATH, 'modulation_distribution.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Null check\n",
    "print('\\nNull values per column:')\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4298d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract NumPy arrays X (signals), Y (labels), Z (SNR)\n",
    "# X shape target: (N, 2, 128)\n",
    "X = np.stack(df['signal'].values, axis=0)\n",
    "Y = df['modulation'].values.astype(str)\n",
    "Z = df['snr'].values.astype(int)\n",
    "\n",
    "print('X shape:', X.shape, '| Y shape:', Y.shape, '| Z shape:', Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005133a",
   "metadata": {},
   "source": [
    "## Raw Data Visualization (SNR = 18 dB)\n",
    "\n",
    "For each modulation type, we visualize: I/Q waveform, amplitude, wrapped phase, and the constellation diagram. We improve line clarity using thicker line widths (no data mutation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bf87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helpers (non-mutating)\n",
    "def _ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def plot_iq_waveform(i, q, title, save_path):\n",
    "    \"\"\"\n",
    "    Plot I and Q waveforms on the same time axis.\n",
    "    Inputs: i, q (1D arrays of shape (128,))\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    t = np.arange(len(i))\n",
    "    plt.plot(t, i, label='I', linewidth=2.0)\n",
    "    plt.plot(t, q, label='Q', linewidth=2.0)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (samples)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend(loc='best')\n",
    "    plt.margins(x=0.02, y=0.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_amplitude(signal_complex, title, save_path):\n",
    "    amp = np.abs(signal_complex)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    t = np.arange(len(amp))\n",
    "    plt.plot(t, amp, color='#4C78A8', linewidth=2.0)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (samples)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.margins(x=0.02, y=0.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_phase(signal_complex, title, save_path):\n",
    "    phase = np.angle(signal_complex)  # wrapped to [-pi, pi]\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    t = np.arange(len(phase))\n",
    "    plt.plot(t, phase, color='#F58518', linewidth=2.0)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (samples)')\n",
    "    plt.ylabel('Phase (rad)')\n",
    "    plt.margins(x=0.02, y=0.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_constellation(i, q, title, save_path):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(i, q, s=12, alpha=0.8, c=np.linspace(0,1,len(i)), cmap='viridis', edgecolors='none')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('In-phase (I)')\n",
    "    plt.ylabel('Quadrature (Q)')\n",
    "    # Add small margins so points are not at edges\n",
    "    x_min, x_max = np.min(i), np.max(i)\n",
    "    y_min, y_max = np.min(q), np.max(q)\n",
    "    dx = (x_max - x_min) * 0.05 + 1e-6\n",
    "    dy = (y_max - y_min) * 0.05 + 1e-6\n",
    "    plt.xlim(x_min - dx, x_max + dx)\n",
    "    plt.ylim(y_min - dy, y_max + dy)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations at SNR = 18 dB, one set per modulation\n",
    "_ensure_dir(VISUALIZATIONS_PATH)\n",
    "SNR_TARGET = 18\n",
    "df_18 = df[df['snr'] == SNR_TARGET]\n",
    "\n",
    "for mod in mods:\n",
    "    sub = df_18[df_18['modulation'] == mod]\n",
    "    if len(sub) == 0:\n",
    "        print(f'No samples for {mod} at SNR={SNR_TARGET} dB')\n",
    "        continue\n",
    "    sig = sub['signal'].iloc[0]  # shape (2, 128)\n",
    "    I = sig[0].astype(float)\n",
    "    Q = sig[1].astype(float)\n",
    "    c = I + 1j * Q\n",
    "    # I/Q waveform\n",
    "    plot_iq_waveform(I, Q, f'I/Q Waveform — {mod} @ {SNR_TARGET} dB', os.path.join(VISUALIZATIONS_PATH, f'iq_waveform_{mod}_snr{SNR_TARGET}.png'))\n",
    "    # Amplitude\n",
    "    plot_amplitude(c, f'Amplitude — {mod} @ {SNR_TARGET} dB', os.path.join(VISUALIZATIONS_PATH, f'amplitude_{mod}_snr{SNR_TARGET}.png'))\n",
    "    # Wrapped Phase\n",
    "    plot_phase(c, f'Wrapped Phase — {mod} @ {SNR_TARGET} dB', os.path.join(VISUALIZATIONS_PATH, f'phase_{mod}_snr{SNR_TARGET}.png'))\n",
    "    # Constellation\n",
    "    plot_constellation(I, Q, f'Constellation — {mod} @ {SNR_TARGET} dB', os.path.join(VISUALIZATIONS_PATH, f'constellation_{mod}_snr{SNR_TARGET}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad0f19",
   "metadata": {},
   "source": [
    "## Exact Stratified Split (80/10/10) by (Modulation, SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eac824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform exact 80/10/10 split per (mod, snr) pair with reproducibility\n",
    "def stratified_split_by_pair(X, Y, Z, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=SEED):\n",
    "    \"\"\"\n",
    "    Exact split per (modulation, SNR) pair using deterministic shuffling.\n",
    "    Fits the specified ratios exactly within each group.\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio - 0.9) < 1e-6 and abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, 'Ratios must sum to 1.0'\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # Indices grouped by (mod, snr)\n",
    "    keys = np.array([Y, Z]).T  # shape (N, 2)\n",
    "    # Build mapping from (mod, snr) -> indices\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)\n",
    "    for idx, (mod, snr) in enumerate(keys):\n",
    "        groups[(mod, int(snr))].append(idx)\n",
    "\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for k, idxs in groups.items():\n",
    "        idxs = np.array(idxs)\n",
    "        rng.shuffle(idxs)\n",
    "        n = len(idxs)\n",
    "        n_train = int(round(train_ratio * n))\n",
    "        n_val = int(round(val_ratio * n))\n",
    "        n_test = n - n_train - n_val\n",
    "        train_idx.extend(idxs[:n_train])\n",
    "        val_idx.extend(idxs[n_train:n_train+n_val])\n",
    "        test_idx.extend(idxs[n_train+n_val:])\n",
    "    return (np.array(train_idx), np.array(val_idx), np.array(test_idx))\n",
    "\n",
    "train_idx, val_idx, test_idx = stratified_split_by_pair(X, Y, Z, 0.8, 0.1, 0.1, SEED)\n",
    "\n",
    "def split_arrays(X, Y, Z, idx):\n",
    "    return X[idx], Y[idx], Z[idx]\n",
    "\n",
    "X_train, y_train, snr_train = split_arrays(X, Y, Z, train_idx)\n",
    "X_val, y_val, snr_val = split_arrays(X, Y, Z, val_idx)\n",
    "X_test, y_test, snr_test = split_arrays(X, Y, Z, test_idx)\n",
    "\n",
    "print('Train:', X_train.shape, '| Val:', X_val.shape, '| Test:', X_test.shape)\n",
    "\n",
    "# Verify proportion per pair\n",
    "def count_per_pair(labels, snrs):\n",
    "    s = pd.Series([f'{l}|{int(s)}' for l, s in zip(labels, snrs)])\n",
    "    return s.value_counts().sort_index()\n",
    "\n",
    "print('\\nCheck per (mod,SNR) counts in each split:')\n",
    "print('Train pairs:', count_per_pair(y_train, snr_train).head())\n",
    "print('Val pairs:', count_per_pair(y_val, snr_val).head())\n",
    "print('Test pairs:', count_per_pair(y_test, snr_test).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441882d",
   "metadata": {},
   "source": [
    "## Label Encoding (fit on y_train only) and One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding and one-hot (no leakage)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)  # fit on train only\n",
    "\n",
    "y_train_int = label_encoder.transform(y_train)\n",
    "y_val_int = label_encoder.transform(y_val)\n",
    "y_test_int = label_encoder.transform(y_test)\n",
    "\n",
    "# One-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "def to_one_hot(y_int, n_classes):\n",
    "    y_oh = np.zeros((len(y_int), n_classes), dtype=np.float32)\n",
    "    y_oh[np.arange(len(y_int)), y_int] = 1.0\n",
    "    return y_oh\n",
    "\n",
    "y_train_oh = to_one_hot(y_train_int, num_classes)\n",
    "y_val_oh = to_one_hot(y_val_int, num_classes)\n",
    "y_test_oh = to_one_hot(y_test_int, num_classes)\n",
    "\n",
    "# Save LabelEncoder\n",
    "with open(os.path.join(PREPROCESSING_OBJECTS_PATH, 'label_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print('Label encoder saved.')\n",
    "print('Classes:', label_encoder.classes_.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909ef02",
   "metadata": {},
   "source": [
    "## Z-Score Normalization (fit on X_train only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606bbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize per-channel using StandardScaler\n",
    "# Reshape to (N*128, 2) to scale channels independently\n",
    "def reshape_for_scaler(X):\n",
    "    N = X.shape[0]\n",
    "    return X.transpose(0, 2, 1).reshape(-1, 2)  # (N,2,128)->(N,128,2)->(N*128,2)\n",
    "\n",
    "def reshape_back_from_scaler(X_scaled_flat, N):\n",
    "    Xs = X_scaled_flat.reshape(N, 128, 2).transpose(0, 2, 1)  # (N,128,2)->(N,2,128)\n",
    "    return Xs\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_flat = reshape_for_scaler(X_train)\n",
    "scaler.fit(X_train_flat)  # fit on train only\n",
    "\n",
    "X_train_scaled = reshape_back_from_scaler(scaler.transform(X_train_flat), X_train.shape[0])\n",
    "X_val_scaled = reshape_back_from_scaler(scaler.transform(reshape_for_scaler(X_val)), X_val.shape[0])\n",
    "X_test_scaled = reshape_back_from_scaler(scaler.transform(reshape_for_scaler(X_test)), X_test.shape[0])\n",
    "\n",
    "# Save scaler\n",
    "with open(os.path.join(PREPROCESSING_OBJECTS_PATH, 'standard_scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print('Standard scaler saved.')\n",
    "\n",
    "print('Scaled shapes:', X_train_scaled.shape, X_val_scaled.shape, X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95eb65",
   "metadata": {},
   "source": [
    "## Reshape to 1D and 2D formats and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D format: (samples, 128, 2)\n",
    "def to_1d_format(X):\n",
    "    return X.transpose(0, 2, 1)  # (N,2,128)->(N,128,2)\n",
    "\n",
    "# 2D format: (samples, 2, 128, 1)\n",
    "def to_2d_format(X):\n",
    "    return X[:, :, :, np.newaxis]  # (N,2,128)->(N,2,128,1)\n",
    "\n",
    "X_train_1d = to_1d_format(X_train_scaled)\n",
    "X_val_1d = to_1d_format(X_val_scaled)\n",
    "X_test_1d = to_1d_format(X_test_scaled)\n",
    "\n",
    "X_train_2d = to_2d_format(X_train_scaled)\n",
    "X_val_2d = to_2d_format(X_val_scaled)\n",
    "X_test_2d = to_2d_format(X_test_scaled)\n",
    "\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_train_1d.npy'), X_train_1d)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_val_1d.npy'), X_val_1d)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_test_1d.npy'), X_test_1d)\n",
    "\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_train_2d.npy'), X_train_2d)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_val_2d.npy'), X_val_2d)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_test_2d.npy'), X_test_2d)\n",
    "\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'y_train.npy'), y_train_oh)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'y_val.npy'), y_val_oh)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'y_test.npy'), y_test_oh)\n",
    "\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'snr_train.npy'), snr_train)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'snr_val.npy'), snr_val)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'snr_test.npy'), snr_test)\n",
    "\n",
    "print('Saved all processed datasets and labels/SNRs.')\n",
    "print('1D shapes:', X_train_1d.shape, X_val_1d.shape, X_test_1d.shape)\n",
    "print('2D shapes:', X_train_2d.shape, X_val_2d.shape, X_test_2d.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
